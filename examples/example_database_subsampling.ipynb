{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: subsampling from newspaper metadata stored in PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how the `subsamplr` package can be used to subsample from records in a relational database containing metadata relating to historical newspaper articles.\n",
    "\n",
    "The database includes information about the year of publication, the word count and the OCR quality of each article, and these variables are used as dimensions for representative subsampling.\n",
    "\n",
    "We want our subsample to be representative in the sense that it preserves the joint distributions, along the three variables of interest, found in the wider dataset from which it is drawn. We also want to be able to condition on those variables, for instance by excluding newspaper articles published before a particular year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install subsamplr and its dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook you will need to install subsamplr and its dependencies. A convenient way to do this is to create a virtual environment, add that environment to Jupyter, and select it as the running kernel. Once this is done, we can import the required classes from subsamplr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from io import StringIO\n",
    "from numpy.random import seed as npseed  # type: ignore\n",
    "\n",
    "os.chdir(\"..\")\n",
    "from subsamplr import BinCollection, DbUnitGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up some configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following YAML-formatted string contains configuration parameters for:\n",
    " - connecting to the database\n",
    " - specifying the subsampling variables of interest, their upper & lower bounds and bin sizes\n",
    " - querying the database to obtain data on each of the variables of interest.\n",
    " \n",
    "**NOTE:** To run this example you will need to enter the database hostname (`db_host`) and your username (`db_user`) in the following configuration parameters. When querying the database (below) you will be prompted to enter your password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_str = \"\"\"\n",
    "    \n",
    "    # Database connection parameters\n",
    "    db_dialect: 'postgresql'\n",
    "    db_host: '<HOSTNAME>'\n",
    "    db_port: 5432\n",
    "    db_user: '<USERNAME>'\n",
    "    db_database: 'newspapers'\n",
    "    \n",
    "    # Subsampling dimensions\n",
    "    variables:\n",
    "        - {name: 'year', class: 'discrete', type: 'int', min: 1800,\n",
    "            max: 1919, discretisation: 1, bin_size: 10}\n",
    "        - {name: 'word_count', class: 'continuous', type: 'int',\n",
    "            min: 0, max: 15000, bin_size: 1000}\n",
    "        - {name: 'ocr_quality_mean', class: 'continuous', type: 'float',\n",
    "            min: 0.6, max: 1, bin_size: 0.1}\n",
    "\n",
    "    queries:\n",
    "        article: |\n",
    "            SELECT\n",
    "                publication.fmp_id as nlp,\n",
    "                publication.title as publication,\n",
    "                publication.location as location,\n",
    "                issue.issue_date as issue_date,\n",
    "                CAST(EXTRACT(YEAR FROM issue.issue_date) AS INTEGER) as year,\n",
    "                issue.input_sub_path as directory_path,\n",
    "                article.word_count as word_count,\n",
    "                article.ocr_quality_mean as ocr_quality_mean,\n",
    "                article.title as article_title,\n",
    "                article.fmp_id as article_fmp_id,\n",
    "                issue.input_sub_path || '/' || article.fmp_id as article_id\n",
    "            FROM\n",
    "                publication,\n",
    "                issue,\n",
    "                article\n",
    "            WHERE\n",
    "                issue.publication_id=publication.id AND\n",
    "                article.issue_id=issue.id;\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data from the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records in the database will be used to generate subsampling \"units\", which in this case represent newspaper articles. We therefore construct a `DbUnitGenerator` object, and pass to it the configuration parameters for connecting to the database.\n",
    "\n",
    "To reduce the time taken to execute the database query, we limit the number of results in this example to 50,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML config.\n",
    "config = yaml.safe_load(StringIO(config_str))\n",
    "\n",
    "# Limit the number of results from the database query.\n",
    "limit = 50000\n",
    "query = config['queries']['article'].rstrip()[:-1] + f\" LIMIT {limit};\"\n",
    "\n",
    "# Construct a database-driven subsampling unit generator.\n",
    "ug = DbUnitGenerator(config['db_dialect'], config['db_host'],\n",
    "                     config['db_port'], config['db_database'],\n",
    "                     config['db_user'])\n",
    "\n",
    "# Fetch the data.\n",
    "df = ug.fetch_data(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each subsampling dimension appears as a column in the data table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database query in the configuration is written such that each of the subsampling dimensions appears as a column in the fetched data.\n",
    "\n",
    "Here the subsampling dimensions are `year`, `word_count` and `ocr_quality_mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data table must also contain a string identifier for each row. Here, the `article_id` serves this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate subsampling units from the table rows and assign to bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the data represents one article, and each of these is a unit in the sample from which we will draw a representative subsample.\n",
    "\n",
    "The first step is to generate the units from the data, and assign them to bins according to their corresponding values along the three subsampling dimensions.\n",
    "\n",
    "To do this we construct a `BinCollection` object, passing in the configuration parameters that specify the subsampling dimensions, their upper and lower bounds and their bin sizes.\n",
    "\n",
    "We then pass the dimensions of the `BinCollection` to the `DbUnitGenerator`, so that it knows what information to extract from the data when converting it into a collection of subsampling units (articles).\n",
    "\n",
    "Each unit consists of a pair of the form `(unit, values)`, where `unit` is a string identifier (the `article_id` in this case) and `values` is a tuple of corresponding values along each of the subsampling dimensions (`year`, `word_count` and `ocr_quality_mean`).\n",
    "\n",
    "This collection of units is then assigned to bins in the `BinCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate newspaper article units from the data and assign to a BinCollection.\n",
    "bc = BinCollection.construct(config, track_exclusions=True)\n",
    "\n",
    "units = DbUnitGenerator.generate_units(\n",
    "    df, unit_id=\"article_id\", variables=bc.dimensions)\n",
    "\n",
    "# Get all of the units from the generator.\n",
    "generated_units = list(units)\n",
    "\n",
    "for unit, values in generated_units:\n",
    "    bc.assign_to_bin(unit, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assignment, the `BinCollection` contains 34,803 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.count_units()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is fewer than the 50,000 rows in the data table. Why? Two reasons:\n",
    "\n",
    "First, the lower and upper bounds of the dimensions in the `BinCollection` were specified in the config parameters:\n",
    "```\n",
    "# Subsampling dimensions\n",
    "variables:\n",
    "    - {name: 'year', class: 'discrete', type: 'int', min: 1800,\n",
    "        max: 1919, discretisation: 1, bin_size: 10}\n",
    "    - {name: 'word_count', class: 'continuous', type: 'int',\n",
    "        min: 0, max: 15000, bin_size: 1000}\n",
    "    - {name: 'ocr_quality_mean', class: 'continuous', type: 'float',\n",
    "        min: 0.6, max: 1, bin_size: 0.1}\n",
    "```\n",
    "and any units whose values fall outside of those bounds are excluded from the collection.\n",
    "\n",
    "Since we set `track_exclusions=True` when constructing the `BinCollection`, we have a record of those exclusions. There are 15,195 of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.count_exclusions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 34,803 units (articles) in the `BinCollection` and 15,195 units in the data were excluded because they fell outside of the bounds of the collection. \n",
    "\n",
    "Together this accounts for 49,998 of the 50,000 rows in the data. What about the other 2 rows?\n",
    "\n",
    "Those rows contained missing values and were therefore never turned into units at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise we have:\n",
    " - 50,000 rows of data\n",
    " - 49,998 rows without any missing data, from each of which a subsampling unit was generated\n",
    " - 34,803 units assigned to bins in the `BinCollection`\n",
    " - 15,195 units excluded from the `BinCollection` as their values are out of range of the configured dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a representative subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a populated `BinCollection`, we can draw a subsample that is representative of the entire collection of units. \n",
    "\n",
    "This is achieved by using the (histogram) counts in each bin as weights for subsampling, and selecting units from each bin at random, according to the weight associated with that bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a subsample of 5000 units.\n",
    "k = 5000\n",
    "seed = 147\n",
    "npseed(seed)\n",
    "\n",
    "sample = bc.select_units(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsample consists of a list of unit identifiers, which in this case identify particular newspaper articles by the combination of their:\n",
    " - seven digit title ID (a.k.a. NLP code)\n",
    " - four digit year of publication\n",
    " - four dight month and day of publication\n",
    " - article ID number within the newspaper issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have constructed a subsample of 5,000 articles, drawn from (a subset of) the 50,000 rows in our data, which is representative of that larger dataset in the sense that the joint distributions along all of the variables of interest (`year`, `word_count` and `ocr_quality_mean`) are preserved in the subsample.\n",
    "\n",
    "Two of the rows of data contained missing values and were discarded. Another 15,195 rows fell outside the bounds of our collection of bins, as defined by our configuration parameters, and were ignored when drawing the sample.\n",
    "\n",
    "The representative sample of size 5,000 was drawn from the remaining 34,803 articles whose values along the dimensions of interest fell within our prescribed bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subsamplr",
   "language": "python",
   "name": "subsamplr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
